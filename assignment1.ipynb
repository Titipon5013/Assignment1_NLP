{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Necessary Libraries",
   "id": "d18ab0fc441ff52d"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-12-10T01:19:44.400661Z",
     "start_time": "2025-12-10T01:19:44.396364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import spacy"
   ],
   "id": "d49ccd140624f20b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "5b47442540429e55",
   "metadata": {},
   "source": "## Setup & Configuration"
  },
  {
   "cell_type": "code",
   "id": "ae5d4ee15efcccba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T01:19:44.801914Z",
     "start_time": "2025-12-10T01:19:44.416602Z"
    }
   },
   "source": [
    "print(\"--- Setting up environments ---\")\n",
    "\n",
    "# NLTK Setup\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# SpaCy Setup\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 2500000\n",
    "except OSError:\n",
    "    print(\"Error: SpaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    exit()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up environments ---\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "4a0c788faff84b2b",
   "metadata": {},
   "source": "## Helper Functions"
  },
  {
   "cell_type": "code",
   "id": "d6c67af24f5b7f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T01:19:44.812722Z",
     "start_time": "2025-12-10T01:19:44.808552Z"
    }
   },
   "source": [
    "def read_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    cleaned = re.sub(r'[^a-z\\s]', '', text_lower)\n",
    "\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    tokens = cleaned.split()\n",
    "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_tokens)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "72e86dcb27ff3d8e",
   "metadata": {},
   "source": "## Processing Functions"
  },
  {
   "cell_type": "code",
   "id": "802c7569a5b8b658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T01:19:44.822452Z",
     "start_time": "2025-12-10T01:19:44.818945Z"
    }
   },
   "source": [
    "def process_nltk(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    words = cleaned.split()\n",
    "\n",
    "    return len(sentences), len(words), sentences, words\n",
    "\n",
    "def process_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentences = blob.sentences\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    words = cleaned.split()\n",
    "\n",
    "    return len(sentences), len(words), sentences, words\n",
    "\n",
    "def process_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    cleaned = clean_text(text)\n",
    "    words = cleaned.split()\n",
    "\n",
    "    return len(sentences), len(words), sentences, words"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "8be794c62a25a57b",
   "metadata": {},
   "source": "## Main Execution"
  },
  {
   "cell_type": "code",
   "id": "59a887932d27e0f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T01:20:24.550008Z",
     "start_time": "2025-12-10T01:19:44.831797Z"
    }
   },
   "source": [
    "def main():\n",
    "    input_filename = 'alice29.txt'\n",
    "    original_text = read_file(input_filename)\n",
    "\n",
    "    if original_text is None:\n",
    "        return\n",
    "\n",
    "    print(f\"File loaded: {input_filename} (Length: {len(original_text)} chars)\")\n",
    "\n",
    "    N_LOOPS = 10\n",
    "    print(f\"\\n[Phase 1] Benchmarking (Running {N_LOOPS} loops per framework)...\")\n",
    "    print(\"Please wait, this might take a moment...\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1. NLTK\n",
    "    t_nltk = timeit.timeit(lambda: process_nltk(original_text), number=N_LOOPS)\n",
    "    avg_nltk = t_nltk / N_LOOPS\n",
    "\n",
    "    s_nltk, w_nltk, sent_list_nltk, word_list_nltk = process_nltk(original_text)\n",
    "    results.append({'Framework': 'NLTK', 'Avg Time (s)': avg_nltk, 'Sentences': s_nltk, 'Words': w_nltk})\n",
    "    print(f\" -> NLTK finished (Avg: {avg_nltk:.4f}s)\")\n",
    "\n",
    "    # 2. TextBlob\n",
    "    t_tb = timeit.timeit(lambda: process_textblob(original_text), number=N_LOOPS)\n",
    "    avg_tb = t_tb / N_LOOPS\n",
    "    s_tb, w_tb, _, _ = process_textblob(original_text)\n",
    "    results.append({'Framework': 'TextBlob', 'Avg Time (s)': avg_tb, 'Sentences': s_tb, 'Words': w_tb})\n",
    "    print(f\" -> TextBlob finished (Avg: {avg_tb:.4f}s)\")\n",
    "\n",
    "    # 3. SpaCy\n",
    "    t_sp = timeit.timeit(lambda: process_spacy(original_text), number=N_LOOPS)\n",
    "    avg_sp = t_sp / N_LOOPS\n",
    "    s_sp, w_sp, _, _ = process_spacy(original_text)\n",
    "    results.append({'Framework': 'SpaCy', 'Avg Time (s)': avg_sp, 'Sentences': s_sp, 'Words': w_sp})\n",
    "    print(f\" -> SpaCy finished (Avg: {avg_sp:.4f}s)\")\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Output 1: time_compares.txt\n",
    "    with open('time_compares.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Framework Performance Comparison (Average of {N_LOOPS} runs)\\n\")\n",
    "        f.write(\"=================================================\\n\\n\")\n",
    "        f.write(df_results.to_string(index=False))\n",
    "\n",
    "    # Output 2: cleaned.txt\n",
    "    cleaned_content = clean_text(original_text)\n",
    "    with open('cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_content)\n",
    "\n",
    "    # Output 3: words.txt\n",
    "    with open('words.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"--- Tokenized Sentences ---\\n\")\n",
    "        for sent in sent_list_nltk:\n",
    "            f.write(str(sent) + \"\\n\")\n",
    "\n",
    "        f.write(\"\\n--- Tokenized Words (Stopwords Removed) ---\\n\")\n",
    "        f.write('\\n'.join(word_list_nltk))\n",
    "\n",
    "    # Output 4: top10words.txt\n",
    "    top_10 = Counter(word_list_nltk).most_common(10)\n",
    "    with open('top10words.txt', 'w', encoding='utf-8') as f:\n",
    "        header = f\"{'Rank':<5} {'Word':<15} {'Frequency':<10}\\n\"\n",
    "        f.write(header)\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for i, (word, freq) in enumerate(top_10, 1):\n",
    "            f.write(f\"{i:<5} {word:<15} {freq:<10}\\n\")\n",
    "\n",
    "    print(\"\\n[Phase 2] Output files generated successfully.\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    if 'get_ipython' not in globals():\n",
    "        print(\"\\n--- Final Comparison Table ---\")\n",
    "        print(df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded: alice29.txt (Length: 148481 chars)\n",
      "\n",
      "[Phase 1] Benchmarking (Running 10 loops per framework)...\n",
      "Please wait, this might take a moment...\n",
      " -> NLTK finished (Avg: 0.0394s)\n",
      " -> TextBlob finished (Avg: 0.0359s)\n",
      " -> SpaCy finished (Avg: 3.5505s)\n",
      "\n",
      "[Phase 2] Output files generated successfully.\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
